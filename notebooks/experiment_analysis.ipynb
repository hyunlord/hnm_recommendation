{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Recommendation Model Experiment Analysis\n",
    "\n",
    "This notebook analyzes the results from model comparison experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest experiment directory\n",
    "experiments_dir = Path('../experiments')\n",
    "experiment_dirs = sorted([d for d in experiments_dir.iterdir() if d.is_dir()], \n",
    "                        key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "\n",
    "if experiment_dirs:\n",
    "    latest_exp_dir = experiment_dirs[0]\n",
    "    print(f\"Latest experiment: {latest_exp_dir.name}\")\n",
    "else:\n",
    "    print(\"No experiments found. Run experiments first!\")\n",
    "    latest_exp_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from all experiments\n",
    "def load_experiment_results(exp_dir):\n",
    "    \"\"\"Load all experiment results from a directory.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_dir in exp_dir.iterdir():\n",
    "        if model_dir.is_dir():\n",
    "            # Try to load results.yaml\n",
    "            results_files = list(model_dir.glob('*_results.yaml'))\n",
    "            if results_files:\n",
    "                with open(results_files[0], 'r') as f:\n",
    "                    data = yaml.safe_load(f)\n",
    "                    data['model'] = model_dir.name\n",
    "                    results.append(data)\n",
    "            \n",
    "            # Try to load results.json as backup\n",
    "            elif (model_dir / 'results.json').exists():\n",
    "                with open(model_dir / 'results.json', 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    results.append(data)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if latest_exp_dir:\n",
    "    results_df = load_experiment_results(latest_exp_dir)\n",
    "    print(f\"Loaded {len(results_df)} experiment results\")\n",
    "    display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main metrics comparison\n",
    "if len(results_df) > 0:\n",
    "    metrics = ['test_map', 'test_recall', 'test_precision', 'test_ndcg']\n",
    "    available_metrics = [m for m in metrics if m in results_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, metric in enumerate(available_metrics):\n",
    "            ax = axes[i]\n",
    "            data = results_df[['model', metric]].dropna()\n",
    "            \n",
    "            bars = ax.bar(data['model'], data[metric])\n",
    "            ax.set_title(f'{metric.upper()} by Model', fontsize=14)\n",
    "            ax.set_xlabel('Model', fontsize=12)\n",
    "            ax.set_ylabel(metric.upper(), fontsize=12)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Color best performer\n",
    "            best_idx = data[metric].idxmax()\n",
    "            bars[best_idx].set_color('red')\n",
    "            \n",
    "            # Add value labels\n",
    "            for j, (idx, row) in enumerate(data.iterrows()):\n",
    "                ax.text(j, row[metric] + 0.001, f'{row[metric]:.4f}', \n",
    "                       ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(available_metrics), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No test metrics found in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary table\n",
    "if len(results_df) > 0 and 'test_map' in results_df.columns:\n",
    "    summary_cols = ['model', 'test_map', 'test_recall', 'test_precision', 'test_ndcg']\n",
    "    summary_cols = [col for col in summary_cols if col in results_df.columns]\n",
    "    \n",
    "    summary_df = results_df[summary_cols].copy()\n",
    "    summary_df = summary_df.round(4)\n",
    "    summary_df = summary_df.sort_values('test_map', ascending=False)\n",
    "    \n",
    "    # Calculate rank for each metric\n",
    "    for metric in ['test_map', 'test_recall', 'test_precision', 'test_ndcg']:\n",
    "        if metric in summary_df.columns:\n",
    "            summary_df[f'{metric}_rank'] = summary_df[metric].rank(ascending=False)\n",
    "    \n",
    "    print(\"Performance Summary (sorted by MAP@12):\")\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Best model analysis\n",
    "    best_model = summary_df.iloc[0]\n",
    "    print(f\"\\nBest Model: {best_model['model']}\")\n",
    "    print(f\"MAP@12: {best_model['test_map']:.4f}\")\n",
    "    print(f\"Recall@12: {best_model.get('test_recall', 'N/A')}\")\n",
    "    print(f\"Precision@12: {best_model.get('test_precision', 'N/A')}\")\n",
    "    print(f\"NDCG@12: {best_model.get('test_ndcg', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time analysis\n",
    "if 'duration' in results_df.columns:\n",
    "    results_df['duration_min'] = results_df['duration'] / 60\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(data=results_df, x='model', y='duration_min')\n",
    "    plt.title('Training Time by Model', fontsize=14)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Training Time (minutes)', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.1f}',\n",
    "                   (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "                   ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Efficiency score (MAP per minute)\n",
    "    if 'test_map' in results_df.columns:\n",
    "        results_df['efficiency'] = results_df['test_map'] / results_df['duration_min']\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.barplot(data=results_df, x='model', y='efficiency')\n",
    "        plt.title('Model Efficiency (MAP per Training Minute)', fontsize=14)\n",
    "        plt.xlabel('Model', fontsize=12)\n",
    "        plt.ylabel('MAP / Training Minute', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of all metrics\n",
    "if len(results_df) > 0:\n",
    "    metrics = ['test_map', 'test_recall', 'test_precision', 'test_ndcg']\n",
    "    available_metrics = [m for m in metrics if m in results_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        # Prepare data for heatmap\n",
    "        heatmap_data = results_df.set_index('model')[available_metrics]\n",
    "        \n",
    "        # Normalize to 0-1 scale for better comparison\n",
    "        heatmap_normalized = (heatmap_data - heatmap_data.min()) / (heatmap_data.max() - heatmap_data.min())\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(heatmap_normalized.T, annot=heatmap_data.T, fmt='.4f', \n",
    "                   cmap='YlOrRd', cbar_kws={'label': 'Normalized Score'})\n",
    "        plt.title('Model Performance Heatmap', fontsize=14)\n",
    "        plt.xlabel('Model', fontsize=12)\n",
    "        plt.ylabel('Metric', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "if len(results_df) > 0 and 'test_map' in results_df.columns:\n",
    "    print(\"Statistical Summary of MAP@12 across models:\")\n",
    "    print(f\"Mean: {results_df['test_map'].mean():.4f}\")\n",
    "    print(f\"Std: {results_df['test_map'].std():.4f}\")\n",
    "    print(f\"Min: {results_df['test_map'].min():.4f}\")\n",
    "    print(f\"Max: {results_df['test_map'].max():.4f}\")\n",
    "    print(f\"Range: {results_df['test_map'].max() - results_df['test_map'].min():.4f}\")\n",
    "    \n",
    "    # Performance improvement over baseline\n",
    "    if 'popularity_baseline' in results_df['model'].values:\n",
    "        baseline_map = results_df[results_df['model'] == 'popularity_baseline']['test_map'].values[0]\n",
    "        \n",
    "        print(f\"\\nImprovement over Popularity Baseline (MAP={baseline_map:.4f}):\")\n",
    "        for _, row in results_df.iterrows():\n",
    "            if row['model'] != 'popularity_baseline':\n",
    "                improvement = ((row['test_map'] - baseline_map) / baseline_map) * 100\n",
    "                print(f\"{row['model']}: {improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "if len(results_df) > 0 and 'test_map' in results_df.columns:\n",
    "    top_models = results_df.nlargest(3, 'test_map')\n",
    "    \n",
    "    print(\"Top 3 Models:\")\n",
    "    for i, (_, model) in enumerate(top_models.iterrows(), 1):\n",
    "        print(f\"{i}. {model['model']}: MAP@12 = {model['test_map']:.4f}\")\n",
    "    \n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"1. Focus on the top performing models for production deployment\")\n",
    "    print(\"2. Consider ensemble methods combining top models\")\n",
    "    print(\"3. Run hyperparameter tuning on the best models\")\n",
    "    print(\"4. Test with larger data samples for more reliable results\")\n",
    "    print(\"5. Analyze failure modes and edge cases for top models\")\n",
    "    \n",
    "    # Model-specific recommendations\n",
    "    best_model_name = top_models.iloc[0]['model']\n",
    "    print(f\"\\nFor the best model ({best_model_name}):\")\n",
    "    \n",
    "    if 'neural' in best_model_name.lower():\n",
    "        print(\"- Try different architectures and layer sizes\")\n",
    "        print(\"- Experiment with different activation functions\")\n",
    "        print(\"- Consider adding attention mechanisms\")\n",
    "    elif 'lightgcn' in best_model_name.lower():\n",
    "        print(\"- Experiment with different number of layers\")\n",
    "        print(\"- Try different graph normalization techniques\")\n",
    "        print(\"- Consider adding edge weights based on interaction strength\")\n",
    "    elif 'wide_deep' in best_model_name.lower():\n",
    "        print(\"- Add more feature engineering for the wide part\")\n",
    "        print(\"- Try different deep network architectures\")\n",
    "        print(\"- Experiment with different feature combinations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to various formats\n",
    "if len(results_df) > 0:\n",
    "    # CSV\n",
    "    results_df.to_csv(latest_exp_dir / 'analysis_results.csv', index=False)\n",
    "    print(f\"Results exported to: {latest_exp_dir / 'analysis_results.csv'}\")\n",
    "    \n",
    "    # Markdown summary\n",
    "    with open(latest_exp_dir / 'analysis_summary.md', 'w') as f:\n",
    "        f.write(\"# Experiment Analysis Summary\\n\\n\")\n",
    "        f.write(f\"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        if 'test_map' in results_df.columns:\n",
    "            summary_df = results_df[['model', 'test_map', 'test_recall', 'test_precision', 'test_ndcg']]\n",
    "            summary_df = summary_df.round(4).sort_values('test_map', ascending=False)\n",
    "            f.write(\"## Performance Summary\\n\\n\")\n",
    "            f.write(summary_df.to_markdown(index=False))\n",
    "        \n",
    "        f.write(\"\\n\\n## Best Model\\n\\n\")\n",
    "        best = results_df.loc[results_df['test_map'].idxmax()]\n",
    "        f.write(f\"**{best['model']}** with MAP@12 = {best['test_map']:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Summary exported to: {latest_exp_dir / 'analysis_summary.md'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}