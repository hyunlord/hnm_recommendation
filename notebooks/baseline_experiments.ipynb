{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from data import HnMLightningDataModule\n",
    "from models import NeuMF\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = HnMLightningDataModule(data_dir='../data', batch_size=2048, num_workers=4)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metric (MAP@12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=12):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=12):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "# Prepare validation data for evaluation\n",
    "val_true_items = dm.val_df.groupby('customer_id')['article_id'].apply(list).to_dict()\n",
    "val_users = list(val_true_items.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline 1: Most Popular Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_items = dm.train_df['article_id'].value_counts().index[:12].tolist()\n",
    "popular_predictions = [popular_items] * len(val_users)\n",
    "actuals = [val_true_items.get(user, []) for user in val_users]\n",
    "\n",
    "map_popular = mapk(actuals, popular_predictions, k=12)\n",
    "print(f\"Most Popular Items Baseline MAP@12: {map_popular:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline 2: NeuMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Model and Trainer Setup ---\n",
    "model = NeuMF(num_users=dm.num_users, num_items=dm.num_items)\n",
    "\n",
    "# Checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='../models/',\n",
    "    filename='best-neumf-model',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1, \n",
    "    accelerator='auto', \n",
    "    logger=True,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# --- 2. Training ---\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 NeuMF Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load the best model ---\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Loading best model from: {best_model_path}\")\n",
    "best_model = NeuMF.load_from_checkpoint(best_model_path)\n",
    "best_model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "\n",
    "# --- 2. Generate Predictions ---\n",
    "all_item_ids_mapped = list(dm.item_map.keys())\n",
    "all_item_indices = torch.tensor(list(dm.item_map.values()), dtype=torch.long).to(device)\n",
    "\n",
    "neumf_predictions = []\n",
    "for user_id_str in tqdm(val_users, desc=\"Generating NeuMF Predictions\"):\n",
    "    user_idx = dm.user_map.get(user_id_str)\n",
    "    if user_idx is not None:\n",
    "        user_tensor = torch.tensor([user_idx] * len(all_item_indices), dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            scores = best_model(user_tensor, all_item_indices)\n",
    "        top_indices = torch.argsort(scores, descending=True)[:12]\n",
    "        predicted_item_ids = [all_item_ids_mapped[i] for i in top_indices.cpu().numpy()]\n",
    "        neumf_predictions.append(predicted_item_ids)\n",
    "    else:\n",
    "        neumf_predictions.append(popular_items) # Fallback for new users\n",
    "\n",
    "# --- 3. Calculate MAP@12 ---\n",
    "map_neumf = mapk(actuals, neumf_predictions, k=12)\n",
    "print(f\"\nNeuMF Model MAP@12: {map_neumf:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from data import HnMLightningDataModule\n",
    "from models import NeuMF\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Install surprise if not already installed\n",
    "try:\n",
    "    from surprise import Dataset, Reader, KNNBasic\n",
    "except ImportError:\n",
    "    !pip install scikit-surprise\n",
    "    from surprise import Dataset, Reader, KNNBasic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = HnMLightningDataModule(data_dir='../data', batch_size=2048, num_workers=4)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metric (MAP@12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=12):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=12):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "# Prepare validation data for evaluation\n",
    "val_true_items = dm.val_df.groupby('customer_id')['article_id'].apply(list).to_dict()\n",
    "val_users = list(val_true_items.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline 1: Most Popular Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Popular Items Baseline MAP@12: 0.003183\n",
      "CPU times: user 3.06 s, sys: 182 ms, total: 3.25 s\n",
      "Wall time: 3.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "popular_items = dm.train_df['article_id'].value_counts().index[:12].tolist()\n",
    "popular_predictions = [popular_items] * len(val_users)\n",
    "actuals = [val_true_items.get(user, []) for user in val_users]\n",
    "\n",
    "map_popular = mapk(actuals, popular_predictions, k=12)\n",
    "print(f\"Most Popular Items Baseline MAP@12: {map_popular:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline 2: NeuMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- 1. Model and Trainer Setup ---\n",
    "model = NeuMF(num_users=dm.num_users, num_items=dm.num_items)\n",
    "trainer = pl.Trainer(max_epochs=1, accelerator='auto', logger=False, enable_checkpointing=False)\n",
    "\n",
    "# --- 2. Training ---\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 NeuMF Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- 1. Setup for Evaluation ---\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "all_item_ids_mapped = list(dm.item_map.keys())\n",
    "all_item_indices = torch.tensor(list(dm.item_map.values()), dtype=torch.long).to(device)\n",
    "\n",
    "# --- 2. Generate Predictions ---\n",
    "neumf_predictions = []\n",
    "for user_id_str in tqdm(val_users, desc=\"Generating NeuMF Predictions\"):\n",
    "    user_idx = dm.user_map.get(user_id_str)\n",
    "    if user_idx is not None:\n",
    "        user_tensor = torch.tensor([user_idx] * len(all_item_indices), dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            scores = model(user_tensor, all_item_indices)\n",
    "        top_indices = torch.argsort(scores, descending=True)[:12]\n",
    "        predicted_item_ids = [all_item_ids_mapped[i] for i in top_indices.cpu().numpy()]\n",
    "        neumf_predictions.append(predicted_item_ids)\n",
    "    else:\n",
    "        neumf_predictions.append(popular_items) # Fallback for new users\n",
    "\n",
    "# --- 3. Calculate MAP@12 ---\n",
    "map_neumf = mapk(actuals, neumf_predictions, k=12)\n",
    "print(f\"NeuMF Model MAP@12: {map_neumf:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
